# 한국어 임베딩 

> 에이콘 출판사

<br>

<br>

## 단어를 벡터로 바꾸면 단어 벡터들 사이의 유사도를 계산할 수 있다. "코사인 유사도"

코사인 유사도 시각화도 가능

<Br>

- 차원 축소

  단어 벡터들을 2차원 시각화도 가능

<Br>

한국어 임베딩 책에서 다루는 데이터의 최소 단위는 "토큰"

### 문장: 토큰의 집합
### 문서: 문장의 집합
### 말뭉치: 문서의 집합
### 토크나이즈: 문장을 토큰으로 분석하는 과정
### 어휘 집합: 말뭉치에 있는 모든 문서를 문장으로 나누고 여기에 토크나이즈를 실시한 후, 중복을 제거한 "토큰들의 집합"

<Br>

<br>

## 벡터가 어떻게 의미를 가지게 되는가

<br>

|             | 백오브워즈bag or words    | 언어 모델language model     | 분포가정distributional hypothesis |
| ----------- | ------------------------- | --------------------------- | --------------------------------- |
|             | 어떤 단어가 많이 쓰였는가 | 어떤 순서로 단어가 쓰였는가 | 어떤 단어가 같이 쓰였는가         |
| 대표 통계량 | TF-IDF                    | -                           | PMI                               |
| 대표 모델   | Deep Averaging Network    | ELMo, GPT                   | Word2Vec                          |

<br>

- ### 백오브워즈 가정

  - 수학에서 백bag이란 중복 원소를 허용한 집합multiset을 의미
  - 순서는 고려하지 않음

- TF-IDF(Term Frequency-Inverse Document Frequency)

- TF: 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지 빈도

- DF: 특정 단어가 나타난 문서의 수

- IDF: 전체 문서 수(N)을 해단 단어의 DF로 나눈 뒤 로그를 취한 값(값이 클수록 특이한 값이라는 의미)

  ​		단어의 주제 예측 능력(해당 단어만 보고 문서의 주제를 가늠해볼 수 있는 정도)과 직결

<br>

- ### 언어 모델
  
  - 단어 시퀀스에 확률을 부여하는 모델
  - 단어 등장 순서를 무시하는 백오브워즈와 달리 언어 모델은 시퀀스 정보를 명시적으로 학습
  - 단어가 n개 주어진 상황일 때, n개 단어가 동시에 나타날 확률, P를 반환
  - 통계 기반의 언어 모델은 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습
  - 잘 학습된 언어 모델이 있다면 어떤 문장이 그럴듯한지(확률 값이 높은지), 주어진 단어 시퀀스 다음 단어는 무엇이 오는 게 자연스러운지 알 수 있다.
  
  <Br>
- n-gram: n개 단어
- 2-gram(바이그램bigram): ex) '난폭, 운전', '눈, 뜨다'
- 3-gram(트라이그램trigram): ex) '짜장, 을, 먹다'
- n-gram: 경우에 따라 n-gram에 기반한 언어 모델을 의미, 말뭉치 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 뜻

<br>

- ### 분포 가정

  - 자연어 처리에서 분포란 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 의미
  - 개별 단어의 분포는 그 단어가 문장 내에 주로 어느 위치에 나타나는지, 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라진다
  - 어떤 단어 쌍pair이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이 분포 가정의 전제
  - 해당 단어가 실제 어떻게 쓰이는지 문맥을 살펴 단어의 의미를 밝히는 것

  <br>

- 형태소morpheme: (언어학) 의미(어휘적, 문법적)를 가지는 최소 단위

- 품사: 단어를 문법적 성질의 공통성에 따라 언어학자들이 몇 갈래로 묶어 놓은 것

- 품사 분류 기준

  <br>

  ### '기능function, 의미meaning, 형식form'
  
  - 기능: 한 단어가 문장 가운데서 다른 단어와 맺는 관계
  - 의미: 단어의 형식적 의미
  
  - 형식: 단어의 형태적 특징
  
  <br>
  
  <br>
  
  - PMI, Pointwise Mutual Information, 점별 상호 정보량: 두 확률 변수 random variable 사이의 상관성을 계량화하는 단위, 두 확률변수가 독립인 경우 그 값은 0.
  - Word2Vec 기법은 PMI 행렬과 깊은 연관이 있다는 논문이 발표되기도 했다
  
  - 백오브워즈 가정, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호 보완적이다.
  
  <br>
  
  <br>
  
  ## 한국어 전처리
  
  - 웹 문서나 json 파일 같은 형태의 데이터 => 순수 텍스트 파일 => 형태소 분석 실시
  
  - ### 형태소 분석 방법
  
    - 국어학 전문가들이 태깅tagging한 데이터로 학습된 모델로 분석하는 **지도 학습 supervised learning**
    - 우리가 가진 말뭉치의 패턴을 학습한 모델을 적용하는 **비지도 학습 unsupervised learning**
  
  - 임베딩을 구축하기 위해서는 말뭉치가 필요
  
  - 임베딩 학습용 말뭉치는 직접 만들거나 웹 스크래핑 scraping으로 모을 수 있음
  
  - 한국어 위키백과에서 raw data를 다운로드
  
  
  <br>
  
  <br>
  
  ## 단어 수준 임베딩
  
  - 예측prediction 기반 모델
    - NPLM, Word2Vec, FastText
  - 가중 임베딩Weighted Embedding
    - 단어 임베딩을 문장 수준으로 확장하는 방법
  
  <br>
  
  - ### NPLM
  
    - Neural Probabilistic Language Model
    - 자연어 처리 분야에 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델
  
    <br>
  
    ## 기존 언어 모델의 단점
  
    - 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값을 0으로 부여
  
    - 물론 백오프back-off나 스무딩smoothing으로 이런 문제를 일부 보완할 수 있지만 완전한 것은 아님
    - 문장의 장기 의존성long-term dependency을 포착해내기 어렵다. (Ex. n-gram의 n을 5 이상으로 길게 설정할 수 없다. n이 커질수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어난다.)
    - 단어/문장 간 유사도를 계산할 수 없다.
  
    <br>
  
  - ### NPLM은 이러한 기존 언어 모델의 한계를 극복한 언어 모델이라는 점에서 의의가 있다.
  
  - ### NPLM 자체로 단어 임베딩 역할을 수행할 수 있다.
  
  <br>
  
  - ### Word2Vec
  
    - Milolov et al. (2013a)이 제안한 CBOW와 Skip-gram 모델. 이 두 모델을 근간으로 하되 네거티브 샘플링negative sampling 등 학습 최적화 기법을 제안한 내용이 핵심
    - **CBOW**는 주변에 있는 문맥 단어context word들을 가지고 타깃 단어target word 하나를 맞추는 과정에서 학습된다.
    - CBOW 쌍 pair: {문맥 단어 4개, 타깃 단어}
  
    <br>
  
    - **Skip-gram 모델**은 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습된다.
    - Skip-gram의 학습 데이터: {타깃 단어, 타깃 직전 두 번째 단어}, {타깃 단어, 타깃 직전 단어}, {타깃 단어, 타깃 다음 단어}, {타깃 단어, 타깃 다음 두 번째 단어} 이렇게 4개쌍이 된다.
    - Skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어, 임베딩 품질이 CBOW보다 좋은 경향이 있다.
  
  <br>
  
  - ### Skip-gram 모델을 중심으로 한 "Word2Vec" 기법
  
    - 포지티즈 샘플positive sample: 타깃 단어(t)와 그 주변에 실제로 등장한 문맥 단어(c) 쌍을 가리킨다.
    - 네거티브 샘플negative sample: 타깃 단어와 그 주변에 등장하지 않은 단어(말뭉치 전체에서 랜덤 추출) 쌍을 의미한다.
    - 윈도우window를 2로 설정: 포지티브 샘플을 만들 때 타깃 단어 앞뒤 두 개씩만 고려한다는 뜻
  
    <br>
  
  - ## FastText
  
    - 각 단어를 문자character 단위 n-gram으로 표현한다. 이 밖의 내용은 Word2Vec와 같다.
  
  