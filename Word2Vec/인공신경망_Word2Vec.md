# 인공신경망

<br>

## 분석 -> 머신러닝

- 도메인 전문가가 아닌데도, 머신러닝이 데이터 패턴을 찾기 때문에 머신러닝이 데이터를 분석할 수 있게 된다.

- 머신러닝에서는 프로그램이 여러 개일 필요가 없다. 

- 데이터 자체로 분석하기 때문에, 대신 데이터가 상당히 많아야 머신러닝이 분석하기 좋다.

  EX) 알파고의 경우도 학습 데이터량이 어마어마하게 많다.

<br>

<br>

### 머신러닝 + 통계학 + 데이터 마이닝 + 신경망

<br>

<br>

![AI](https://user-images.githubusercontent.com/57430754/73703659-066e8800-4734-11ea-8027-d68a5328b88b.jpg)

<br>

<br>

- 뉴런이 1개일 때는 '선형모델'만 쓴다.

<BR>

<br>

## 인공신경망(선형, 비선형)

### 인공신경망은 deepLearning에 필수다!

<br>

- 입력층  -> w값 -> 중간층(노드 또는 뉴런) -> 출력층

- w 값을 최적화 하는 cost function을 구하라! (이때 미분을 사용!)

- 들어오는 데이터에 정규화(표준화); 0-1 사이의 데이터  를 시켜야 그래프가 울퉁불퉁하지 않고, 곱게 나온다.

<br>

- **단층 퍼셉트론**: 입력층 -> 출력층

>  입력층 여러개 -> 시그모이드 함수를 넣어 출력값을 뽑아내는 것!- 
>
> 입력, 출력층이라 '선형'밖에 안 됨!

<br>

- **다층 퍼셉트론**: 입력층 -> 은닉층 -> 출력층 

  > MLP; Multi layers perceptron
  >
  > 노드를 두 개를 둘 수 있음.
  >
  > 각 노드 안에 ' 시그마| f '
  >
  > 각 노드 계산 값을 합산해서 출력값에!  (각 노드값이 그래프 위에 그려짐)
  >
  > ***직관적 설명: 동일한 데이터를 은닉층에서 다르게 보는 것. 그리고 이것을 합산하면, (수학상으로는 w값의 변화) 또 다른 데이터를 볼 수 있다는 것!***
  >
  > <br>
  >
  > 히든 노드 1개: 선형
  >
  > 히든 노드 2개: 평행
  >
  > 히든 노드 3개:  '다각형' (**비선형**)
  >
  > 히든 노드 4개 이상: '곡률'
  >
  > -> 깊이가 깊어질수록 '비선형' 모델! 
  >
  > -> 인공신경망은 다양한 형태의 벡터 모양을 만들어낸다.
  >
  > -> 우리가 생각하지 못한 아주 다양한 형태의 다항식이 나오고, 생각지도 못한 데이터 결과가 나올 수 있다.
  >
  > -> 비선형으로 데이터를 정형화할 수 있기 때문에, SVM과 인공신경망은 유용하게 쓰인다.
  
  <br>

* 층이 많고, 노드가 많아지면 속도가 어마어마하게 느려진다.
* 손실함수 Cost Function == Loss Function
* 햇 기호 y : 예측치
* Hidden Layer가 3층 이상이면, Deep Neural Network (DNN). (굳이 안 나눠도 됨.)
* 딥러닝: 다층 퍼셉트론(MLP) & **심층 퍼셉트론(DNN)**
* 다층 퍼셉트론과 비슷하게 신경망 모델을 하는 **'Word2Vec'**
* 층이 많아지면 w 연산 속도가 엄청 떨어지고, 오버피팅이 생긴다. 같은 데이터를 또 보고 또 보고 하는 것. (결정 트리와 다를 게 없다.) 정확할 수는 있지만, 층이 많아지면 w값을 구하기가 너무 어렵다!! => **역전파 알고리즘으로 해결!!**

<BR>

<br>

# Word2Vec

[Word2Vec]([https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/](https://ratsgo.github.io/from frequency to semantics/2017/03/30/word2vec/))

- word2vec 의 모든 'w' 값은 공유한다.
- w 조정이 많이 되야 학습이 잘 되어 있다고 본다.

<br>

<br>

![도식화](https://user-images.githubusercontent.com/57430754/73716073-5ad82e80-4759-11ea-845c-4eb1c25db1b1.png)

<br>

<br>

### 원핫인코딩이 새로운임베딩으로

![원핫인코딩이 새로운임베딩으로](https://user-images.githubusercontent.com/57430754/73715408-38ddac80-4757-11ea-95a1-5d82d2989804.png)

<br>

<br>

### Skip-Gram

![skip-gram](https://user-images.githubusercontent.com/57430754/73716052-4f850300-4759-11ea-86ee-c521445c005c.png)
